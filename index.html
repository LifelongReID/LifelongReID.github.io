<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Lifelong Person Re-Identification</title>

    <!-- Meta tags for search engines to crawl -->
    <meta name="robots" content="index,follow">
    <meta name="description"
          content="We introduce a new lifelong learning setting of person re-identification (LReID)"

    >
    <meta name="keywords" content="Lifelong Learning; Person Re-Identification">
    <link rel="author" href="https://liacs.leidenuniv.nl/~pun">

    <!-- Fonts and stuff -->
    <link href="./css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="./project.css" media="screen">
    <link rel="stylesheet" type="text/css" media="screen" href="./iconize.css">
    <script async="" src="./prettify.js"></script>

</head>

<body>
<div id="content">
    <div id="content-inner">

        <div class="section head">
            <h1>A Memorizing and Generalizing Framework for <br> Lifelong Person Re-Identification</h1>

            <div class="authors">
                <a href="https://liacs.leidenuniv.nl/~pun/">Nan Pu</a><sup>1</sup>&nbsp;
                <a href="http://zhunzhong.site/">Zhun Zhong</a><sup>2</sup>&nbsp;
                <a href="https://disi.unitn.it/~sebe/">Nicu Sebe</a><sup>2</sup>&nbsp;
                <a href="https://liacs.leidenuniv.nl/~lewms/">Michael Lew</a><sup>1</sup>&nbsp;
            </div>

            <div class="affiliations">
                1. LIACS Media Lab, Leiden University <br>
                2. Department of Information Engineering and Computer Science, University of Trento
            </div>

            <!-- <div class="affiliations">
                enrico.fini@unitn.it&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enver.sangineto@unitn.it&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stephane.lathuiliere@telecom-paris.fr<br>
                zhunzhong007@gmail.com&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.nabi@sap.com&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e.ricci@unitn.it
            </div> -->
        </div>

        <center><img src="./img/pipeline_v4.png" border="0" width="90%"><br>
            Illustration of Lifelong Person Re-Identification (LReID).
        </center>

        <div class="section abstract">
            <h2>Abstract</h2>
            <br>
            <p>
                In this paper, we introduce a challenging yet practical task for person re-identification (ReID), named lifelong person re-identification (LReID), which aims to continuously train a ReID model across multiple domains and the trained model is required to generalize well on both seen and unseen domains. It is therefore critical to learn a ReID model that can learn a generalized representation without forgetting knowledge of seen domains. In this paper, we propose a new MEmorizing and GEneralizing framework (MEGE) for LReID, which can jointly prevent the model from forgetting and improve its generalization ability. Specifically, our MEGE is composed of two novel modules, \textit{i.e.}, Adaptive Knowledge Accumulation (AKA) and differentiable Ranking Consistency Distillation (RCD). Taking inspiration from the cognitive processes in the human brain, we endow AKA with two special capacities, knowledge representation and knowledge operation by graph convolution networks. AKA can effectively mitigate catastrophic forgetting on seen domains while improving the generalization ability to unseen domains. By considering the ranking factor that is specifically important in ReID, RCD is designed to distill the ranking knowledge in a differentiable manner, which can further prevent the catastrophic forgetting. To supporting the study of LReID, we build a new and large-scale benchmark with two practical evaluation protocols that consider the metrics of non-forgetting and generalization. Experiments demonstrate that 1) our MEGE framework can effectively improve the performance on seen and unseen domains under the domain-incremental learning constraint, and that 2) the proposed MEGE outperforms state-of-the-art competitors by large margins.
            </p>

        </div>


        <div class="section method">
            <h2>Method Overview</h2>
            <br>
            <center><img src="./img/framework_v4.png" border="0" width="90%"><br>
                Overview of Entropy-based Uncertainty Modeling and Self-training framework.
            </center>
        </div>

<!--        <div class="section results">-->
<!--            <h2>Results</h2>-->
<!--            <br>-->
<!--            <center><img src="./img/ncdss_pascal.png" border="0" width="55%"><br>-->
<!--                Ablation studies of different components in EUMS framework.-->
<!--            </center>-->

<!--            <center><img src="./img/ncdss_coco.png" border="0" width="40%"><br>-->
<!--                Performance on COCO-20^i benchmark.-->
<!--            </center>-->

<!--            <center><img src="./img/visual_labels.png" border="0" width="80%"><br>-->
<!--                Visualization of entropy maps and pseudo-labels.-->
<!--            </center>-->

<!--            <center><img src="./img/visual_comparsion.png" border="0" width="80%"><br>-->
<!--                Qualitative comparison of segmentation results in PASCAL VOC 2012 validation set.-->
<!--            </center>-->
<!--        </div>-->

        <br>

        <!--=================Materials==========================-->
        <div class="section materials" , id="materials">
            <h2>Materials</h2>
            <table width="80%" align="center" border=none cellspacing="0" cellpadding="30">
                <tr>
                    <td width="60%">
                        <center>
                            <a href="https://arxiv.org/pdf/2103.12462.pdf" target="_blank" class="imageLink"><img
                                    src="./img/all_pages.png" , width="45%"></a><br><br>
                            <a href="https://arxiv.org/pdf/2103.12462.pdf" target="_blank">Paper</a>
                        </center>
                    </td>
                    <td width="40%" valign="middle">
                        <center>
                            <a href="" target="_blank"
                               class="imageLink"><img
                                    src="./icon_github.png" , width="50%"></a><br><br>
                            <a href="https://github.com/TPCD/LifelongReID" target="_blank">Codes</a>
                        </center>
                    </td>
                </tr>
            </table>
        </div>

        <br>

<!--        <div class="section citation">-->
<!--            <h2>Citation</h2>-->
<!--            <div class="section bibtex">-->
<!--      <pre>-->
<!--@inproceedings{zhao2022ncdss,-->
<!--title={Novel Class Discovery in Semantic Segmentation},-->
<!--author={Zhao, Yuyang and Zhong, Zhun and Sebe, Nicu and Lee, Gim Hee},-->
<!--booktitle={Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},-->
<!--year={2022}}-->
<!--    </pre>-->
<!--            </div>-->
<!--        </div>-->

<!--        <div class="section relatedwork">-->
<!--            <h2>Related Work</h2>-->
<!--            <br>-->

<!--            <div class="citation">-->
<!--                <a href="https://arxiv.org/abs/2108.08536">-->
<!--                    A Unified Objective for Novel Class Discovery. ICCV,-->
<!--                    2021.-->
<!--                </a>-->
<!--                <br>-->
<!--            </div>-->

<!--            <div class="citation">-->
<!--                <a href="https://arxiv.org/abs/2004.05551">-->
<!--                    OpenMix: Reviving Known Knowledge for Discovering Novel Visual Categories in An Open World. CVPR,-->
<!--                    2021.-->
<!--                </a>-->
<!--                <br>-->
<!--            </div>-->

<!--            <div class="citation">-->
<!--                <a href="https://arxiv.org/abs/2004.05551">-->
<!--                    Neighborhood Contrastive Learning for Novel Class Discovery. CVPR, 2021.-->
<!--                </a>-->
<!--                <br>-->
<!--            </div>-->

<!--            <div class="citation">-->
<!--                <a href="https://openreview.net/forum?id=BJl2_nVFPB">-->
<!--                    Automatically discovering and learning new visual categories with ranking statistics. ICLR, 2020.-->
<!--                </a>-->
<!--                <br>-->
<!--            </div>-->

<!--            <div class="citation">-->
<!--                <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Han_Learning_to_Discover_Novel_Visual_Categories_via_Deep_Transfer_Clustering_ICCV_2019_paper.pdf">-->
<!--                    Learning to discover novel visual categories via deep transfer clustering. ICCV, 2019.-->
<!--                </a>-->
<!--                <br>-->
<!--            </div>-->

<!--        </div>-->

</body>
</html>
